\subsection{Off-Policy algorithms}
In the off-policy setting, there are two stationary Markov policies, one used
to generate the data, called the behavior policy and another one called the target policy whose value function we seek to estimate. The two policies can be completely arbitrary but subject to some constraints. The behavior policy must be soft, that is it must
have a non-zero probability of selecting every action in
each state. Some algorithms require even weaker constraints on the behavior policy that it can be stationary and non-starving. 

Off-policy evaluation is difficult because there is a mismatch of distributions. Since the learner has to estimate the target policy but is only goven samples from the behavior policy. A classical way of handling such situations comes from \citet{DBLP:books/lib/Rubinstein81} by the way of \textit{Importance Sampling}. Several interesting algorithms in the Reinforcement Learning setting have been proposed for off-policy evaluation incorporating Importance Sampling. These Per-Decision Importance Sampling \citep{DBLP:conf/icml/PrecupSS00}, Per-Decision Weighted Importance Sampling \citep{DBLP:conf/icml/PrecupSS00}, Doubly Robust Importance \citep{DBLP:journals/corr/JiangL15} Sampling and Weighted Doubly Robust Importance Sampling \citep{DBLP:conf/icml/ThomasB16}. 

Fitted Q iteration (FQI) is a batch RL algorithm whose main fea-
ture lies in the way that it handles the experience \citep{DBLP:journals/jmlr/ErnstGW05}. Unlike incremental algorithms like Watkin's Q-learning \citep{DBLP:journals/ml/WatkinsD92}, FQI uses the complete set of transitions each time that updates the estimation of the optimal Q-function. Although this process involves more computation, it allows to extract more information from the stored experience. Conse-
quently, FQI is more data-efficient than other RL algorithms. This feature makes FQI a very suitable algorithm in many application domains. In certain scenarios it is quite expensive to conduct an experiment with respect to both money and time. For, example administering a dose to a patient and waiting to observe its effect. Thus, reducing the quantity of data required by the algorithm can be crucial.

\subsection{Value Function based methods}


\subsection{Policy Gradient Methods}


\subsection{Using Linear and Non-Linear function approximation}