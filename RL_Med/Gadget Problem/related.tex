Existing algorithms for the piecewise i.i.d bandit setting can be broadly divided into two categories, passively adaptive and actively adaptive strategies. Passively adaptive strategies like Discounted UCB (DUCB) \citep{kocsis2006discounted}, Sliding Window UCB (SWUCB) \citep{garivier2011upper} and Discounted Thompson Sampling (DTS) \citep{raj2017taming} do not actively try to locate the changepoints but rather try to minimize their losses by focusing on past few observations. In \citet{garivier2011upper} the authors showed that the regret upper bound of DUCB and SWUCB are respectively $O( \sqrt{GT}\log T)$ and $O(\sqrt{GT\log T} )$, where $G$ is the total number of changepoints and $T$ is the time horizon which are known apriori. Furthermore, \citet{garivier2011upper} proved that the cumulative regret in this setting for a policy $\pi$ for two changepoints   is lower bounded in the order of $\Omega( \sqrt{T})$. The actively adaptive strategies like Adapt-EVE \citep{hartland2007change}, Windowed-Mean Shift \citep{yu2009piecewise}, EXP3.R  \citep{allesiardo2017non}, CUSUM \citep{liu2017change} try to locate the changepoints and restart the chosen bandit algorithms. The regret bound of Adapt-EVE and DTS is still an open problem, whereas the regret upper bound of EXP3.R is $O(G\sqrt{T\log T})$ and that of CUSUM is $O( \sqrt{GT\log({T}/{G})})$. The regret bound of CUSUM is restricted to only Bernoulli distributions. We also implement two oracle algorithms, Oracle UCB1 (OUCB1) and Oracle Thompson Sampling (OTS) which are actively adaptive algorithms with the knowledge of the changepoints. They are restarted at the changepoints without any delay with all their past history of actions and rewards erased. OUCB1  has a regret upper bound of $O(\sqrt{GT\log({T}/{G})})$ (see Proposition \ref{psbandit:Prop:1}, Appendix \ref{proof:Proposition:1}). An extended discussion on related work can be found in Appendix \ref{sec:appendix:related}.


